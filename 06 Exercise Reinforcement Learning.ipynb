{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Reinforcement Learning Moon Lander (10 points)\n",
    "\n",
    "\n",
    "The goal of this exercise is to work with reinforcement learning models and get a basic understanding of the topic. We will first develop controlers for the simple cart pole model and later for the lunar lander.\n",
    "Neil Armstrong was the first to control a lunar lander in 1969. See a [video](https://youtu.be/xc1SzgGhMKc?t=520) about this masterpiece.\n",
    "Luckily, we do not have to go to the moon, but can do our experiments in simulation based on the [Openai gym](https://gym.openai.com/) software.\n",
    "\n",
    "\n",
    "**NOTE**: if openai gym does not install in anaconda, please install the following packages **in your conda environment** using the following commands:\n",
    "\n",
    "```\n",
    "conda install swig\n",
    "pip install gym\n",
    "pip install box2d-py\n",
    "pip install pyglet\n",
    "```\n",
    "\n",
    "**NOTE**: it can happend that the rendering window does not show up or close properly. In this case please check your environment and look for a solution and post it in the forum.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Document your results by simply adding a markdown cell or a python cell (as comment) and writing your statements into this cell. For some tasks the result cell is already available.\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ditomax/mlexercises/blob/master/06%20Exercise%20Reinforcement%20Learning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not running on google colab\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Prepare colab\n",
    "#\n",
    "COLAB=False\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    print(\"running on google colab\")\n",
    "    COLAB=True\n",
    "    !pip install gym\n",
    "    !pip install box2d-py\n",
    "    !sudo apt-get install -y xvfb ffmpeg x11-utils\n",
    "    !pip install -q 'imageio==2.4.0'\n",
    "    !pip install -q PILLOW\n",
    "    !pip install -q 'pyglet==1.3.2'\n",
    "    !pip install -q pyvirtualdisplay\n",
    "    \n",
    "except:\n",
    "    print(\"not running on google colab\")\n",
    "\n",
    "\n",
    "#\n",
    "# Turn off errors and warnings (does not work sometimes)\n",
    "#\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "#\n",
    "# Import modules\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    import PIL.Image\n",
    "    import pyvirtualdisplay\n",
    "    # Set up a virtual display for rendering OpenAI gym environments.\n",
    "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Some print options for numpy\n",
    "#\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting notebook with tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tensorflow\n",
    "#\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR )\n",
    "\n",
    "\n",
    "#\n",
    "# Check version\n",
    "#\n",
    "print('starting notebook with tensorflow version {}'.format(tf.version.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control system with random controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task\n",
    "\n",
    "Run this basic cart pole example with a randome controller (env.action_space.sample()) (2 point) \n",
    "\n",
    "Find out how the example works and what the basic functions of gym are. \n",
    "\n",
    "Comment **each line** of the code with python comments. \n",
    "\n",
    "Find out what the *observation* and *action* values for the cart-pole mean and how the reward is generated.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pos:-0.06 pole angle:0.02 reward:1.0 cr:4.0 d:False   a:1 restarts:5       \n",
      "longest run 28 with 5 restarts\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Result: comments in code\n",
    "#\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "cumulated_reward = 0\n",
    "restart_count = 0\n",
    "longest_run = 0\n",
    "cycle_count = 0\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    if not COLAB:\n",
    "        env.render(mode='close')\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, done, info = env.step( action )\n",
    "    \n",
    "    cumulated_reward += reward\n",
    "    cycle_count += 1\n",
    "    \n",
    "    print( '\\r', 'pos:{:.2f} pole angle:{:.2f} reward:{} cr:{} d:{}   a:{} restarts:{}     '.format(observation[2],observation[0],reward,cumulated_reward,done,action, restart_count), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        cumulated_reward = 0\n",
    "        longest_run = max(longest_run,cycle_count)\n",
    "        cycle_count = 0\n",
    "        restart_count += 1\n",
    "        \n",
    "    # some delay for display to catch up\n",
    "    time.sleep(0.05)\n",
    "      \n",
    "env.close()\n",
    "\n",
    "print( '\\nlongest run {} with {} restarts'.format( longest_run, restart_count) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic on-off control strategy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task\n",
    "\n",
    "Lets attempt to control the cart pole with a simple on-off control strategy. (2 Points)\n",
    "    \n",
    "Reading the [documentation](https://github.com/openai/gym/wiki/CartPole-v0) of this gym we find that it has two actions (push cart left = 0 and push cart right = 1). \n",
    "    \n",
    "So, one idea could be to just look at the pole's angle and push the cart left if the pole leans to the left and vice versa. Give it a try.\n",
    "    \n",
    "**Note:** the angle of the pole is negative if the pole is right from the center.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " position:-0.07 pole angle:0.03 r:1.0 cr:100.0 d:False   a:1   \n",
      "longest run 11 with 10 restarts\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "cumulated_reward = 0\n",
    "pole_angle = 0\n",
    "last_action = -1\n",
    "\n",
    "restart_count = 0\n",
    "longest_run = 0\n",
    "cycle_count = 0\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    if COLAB:\n",
    "        PIL.Image.fromarray(env.render())\n",
    "    else:\n",
    "        env.render(mode='close')\n",
    "    \n",
    "    #\n",
    "    # action values: 0 push cart to the left, 1 push cart to the right\n",
    "    \n",
    "    # TASK: implement your control strategy here\n",
    "    \n",
    "    if ...:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "\n",
    "    observation, reward, done, info = env.step( action )\n",
    "    cumulated_reward += reward\n",
    "\n",
    "    pole_angle = observation[0]\n",
    "    cycle_count += 1\n",
    "    last_action = action\n",
    "\n",
    "        \n",
    "    print( '\\r', 'position:{:.2f} pole angle:{:.2f} r:{} cr:{} d:{}   a:{}   '.format(observation[2],observation[0],reward,cumulated_reward,done,action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        cululated_reward = 0\n",
    "        longest_run = max(longest_run,cycle_count)\n",
    "        cycle_count = 0\n",
    "        restart_count += 1\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.05)\n",
    "      \n",
    "env.close()\n",
    "\n",
    "print( '\\nlongest run {} with {} restarts'.format( longest_run, restart_count) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Controller \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task\n",
    "\n",
    "Now lets build a first version based on advanced RL technique, the Deep Q-Network. \n",
    "    \n",
    "http://arxiv.org/pdf/1312.5602.pdf\n",
    "\n",
    "http://arxiv.org/abs/1509.06461\n",
    "\n",
    "    \n",
    "With DQN, a neural network is trained to estimate the best action for a state based on the Q-learning concept.\n",
    "\n",
    "The code is based on the work by Greg Surma and it can be found [here](https://github.com/gsurma/cartpole).\n",
    "\n",
    "Please go through the code and answer the questions in the comments of the code (marked by TASK). (2 Points)\n",
    "\n",
    "**Note**: Place your answer as comment below the questions.\n",
    "\n",
    "**Note**: This implementation of the DQN does not completely correspond to commonly used DQN implementations. It contains some artistic extensions :-)    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "\n",
    "prefix = 'results/04_dqn_'\n",
    "\n",
    "# sources:\n",
    "# hyperparameters from https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055\n",
    "# further info from https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "\n",
    "\n",
    "GAMMA = 0.8\n",
    "#LEARNING_RATE = 0.00025\n",
    "LEARNING_RATE = 0.007\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "EXPLORATION_MAX = 0.70\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_DECAY = 0.97\n",
    "\n",
    "class DQNControl:\n",
    "\n",
    "    def __init__(self, observation_space, action_space,layout=[24,24],name='nona'):\n",
    "        \n",
    "        print ('building DQN model with observation space {} and action space {} layer {} name {}'.format(observation_space, action_space,layout,name) )\n",
    "        \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        #\n",
    "        # TASK: what is the function of a deque?\n",
    "        # ...\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.name = name\n",
    "        \n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(layout[0], input_shape=(observation_space,), activation=\"relu\", kernel_initializer=init ))\n",
    "        self.model.add(Dense(layout[1], activation=\"relu\", kernel_initializer=init))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\", kernel_initializer=init))\n",
    "        self.model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam( learning_rate=LEARNING_RATE, clipnorm=1.0 ))\n",
    "\n",
    "        \n",
    "    def save(self):\n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        model_json = self.model.to_json()\n",
    "        with open( modelName , \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights( weightName )\n",
    "        print(\"saved model to disk as {} {}\".format(modelName,weightName))\n",
    "\n",
    "        \n",
    "    def load(self):    \n",
    "        modelName = prefix + self.name + \"model.json\"\n",
    "        weightName = prefix + self.name + \"model.h5\"\n",
    "        json_file = open(modelName, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(weightName)\n",
    "        print(\"loaded model from disk file {} {}\".format(modelName,weightName) )\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "    def action(self,state):\n",
    "        # delivers the prediction for the Q function (best action for the state)\n",
    "        q_values = self.model.predict(state)\n",
    "        #\n",
    "        # TASK: what is the idea behind this step (argmax of q_values)?\n",
    "        # ....\n",
    "        #\n",
    "        return np.argmax(q_values[0])\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        # delivers either a random action OR\n",
    "        # TASK: what is the purpose of this if statement\n",
    "        # ....\n",
    "        #\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "\n",
    "        return self.action(state)\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        # build a training patch with all batch samples\n",
    "        batch_state = []\n",
    "        batch_q_values = []\n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            q_value_update = reward\n",
    "            if not done:\n",
    "                #\n",
    "                # TASK: give an explanation for the formula of the update of the Q-value\n",
    "                # ...\n",
    "                #\n",
    "                q_value_update = reward + ( GAMMA * np.amax( self.model.predict(state_next)[0] ) ) \n",
    "            \n",
    "            q_values = self.model.predict(state)\n",
    "            \n",
    "            q_values[0][action] = q_value_update\n",
    "            batch_state.append(state[0])\n",
    "            batch_q_values.append(q_values[0])\n",
    "        \n",
    "        x= np.array(batch_state)    \n",
    "        y= np.array(batch_q_values)    \n",
    "        \n",
    "        self.model.fit(x, y, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "    def close_episode(self):\n",
    "        #\n",
    "        # TASK: what is going on here?\n",
    "        # ...\n",
    "        #\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        \n",
    "\n",
    "\n",
    "def trainDQN(env,episodes=50,layout=[24,24], name='nona', termination_reward=None, termination_runs=None, termination_runs_reward=None ):\n",
    "    \n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    dqn_solver = DQNControl(observation_space, action_space,layout,name)\n",
    "    \n",
    "    history = []\n",
    "    run = 0\n",
    "    \n",
    "    accumulated_reward = 0\n",
    "    sliding_accumulated_reward = 0\n",
    "    \n",
    "    while run < episodes:\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            env.render(mode='close')\n",
    "            \n",
    "            action = dqn_solver.act(state)\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            accumulated_reward += reward\n",
    "            \n",
    "            if not (termination_runs is None) and step > termination_runs:\n",
    "                terminal = True\n",
    "                if not (termination_runs_reward is None):\n",
    "                    reward = termination_runs_reward\n",
    "            else:\n",
    "                if terminal and not (termination_reward is None):\n",
    "                    reward = termination_reward\n",
    "            \n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            state = state_next\n",
    "            \n",
    "            if terminal:\n",
    "                \n",
    "                sliding_accumulated_reward = sliding_accumulated_reward * 0.9 + accumulated_reward * 0.1\n",
    "                \n",
    "                print ( '\\r', 'episode: {}, exploration: {:.3f}, score: {} sliding score {}'.format(run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward), end='' )\n",
    "                \n",
    "                history.append([run,dqn_solver.exploration_rate,accumulated_reward,sliding_accumulated_reward,step])\n",
    "                \n",
    "                accumulated_reward = 0\n",
    "                break\n",
    "            \n",
    "            dqn_solver.experience_replay()\n",
    "        \n",
    "        \n",
    "        dqn_solver.close_episode()\n",
    "        \n",
    "        \n",
    "        run += 1\n",
    "\n",
    "    env.close()\n",
    "    return dqn_solver,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create new environment\n",
    "#\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building DQN model with observation space 4 and action space 2 layer [24, 24] name cartdqn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 12:38:17.777921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 12:38:18.338761: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-11-16 12:38:18.356406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 4050060000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " episode: 20, exploration: 0.381, score: 41.0 sliding score 14.052373748577093"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train model using trainDQN function\n",
    "#\n",
    "control,history = trainDQN(env=env,episodes=50,layout=[24,24],name='cartdqn',termination_reward=-300,termination_runs=100,termination_runs_reward=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save model for later\n",
    "#\n",
    "control.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create dataframe from history\n",
    "#\n",
    "df = pd.DataFrame(history,columns=['run','exploration_rate','accumulated_reward','sliding_accumulated_reward','step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exploration_rate'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accumulated_reward'].plot()\n",
    "df['sliding_accumulated_reward'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test the DQL control\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "control = DQNControl(observation_space, action_space, name='cartdqn')\n",
    "control.load()\n",
    "\n",
    "state = env.reset()\n",
    "cumulated_reward = 0\n",
    "restart_count = 0\n",
    "longest_run = 0\n",
    "cycle_count = 0\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    env.render(mode='close')\n",
    "\n",
    "    # get the next action from the controler\n",
    "    action = control.action( np.reshape(state, [1, observation_space] ) )\n",
    "    \n",
    "    # apply action to environment\n",
    "    observation, reward, done, _ = env.step( action )\n",
    "    \n",
    "    # check status\n",
    "    cumulated_reward += reward\n",
    "    cycle_count += 1\n",
    "    state = observation\n",
    "        \n",
    "    print( '\\r', 'position:{:.2f} pole angle:{:.2f} reward:{} cumulated reward:{} d:{}   a:{}    '.format(observation[2],observation[0],reward,cumulated_reward,done, action), end='' )\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        cumulated_reward = 0\n",
    "        longest_run = max(longest_run,cycle_count)\n",
    "        cycle_count = 0\n",
    "        restart_count += 1\n",
    "\n",
    "    # some delay important for display to catch up\n",
    "    time.sleep(0.05)\n",
    "      \n",
    "env.close()\n",
    "\n",
    "print( '\\nlongest run {} with {} restarts'.format( longest_run, restart_count) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander problem\n",
    "\n",
    "How we are looking into the lunar lander problem. We reuse the DQN controller from above with different parameters. Play with this problem and get an understanding of the rewards. Configuration is taken from [2]. A general discussion about this approach was published in [1].\n",
    "\n",
    "- [1] https://www.researchgate.net/publication/333145451_Deep_Q-Learning_on_Lunar_Lander_Game\n",
    "- [2] https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create new environment for lunar lander\n",
    "#\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control,history = trainDQN(env=env,episodes=150,layout=[64,32],name='lunar',termination_reward=None,termination_runs=150,termination_runs_reward=-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later\n",
    "control.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[2].plot()\n",
    "df[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task\n",
    "\n",
    "Implement an improved controler for the lunar lander (4 points) \n",
    "\n",
    "Search the internet for leadboards for lunar lander and try to implement one of the best solutions. Select your solution by simplicity and clarity of code. Comment the code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Result: implementation of improved controler\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
